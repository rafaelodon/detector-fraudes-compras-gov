<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
	font-size: 14px;
	padding: 0 12px;
	line-height: 22px;
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}


body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	color: #4080D0;
	text-decoration: none;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

h1 code,
h2 code,
h3 code,
h4 code,
h5 code,
h6 code {
	font-size: inherit;
	line-height: auto;
}

a:hover {
	color: #4080D0;
	text-decoration: underline;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left: 5px solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 14px;
	line-height: 19px;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

.mac code {
	font-size: 12px;
	line-height: 18px;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

/** Theming */

.vscode-light,
.vscode-light pre code {
	color: rgb(30, 30, 30);
}

.vscode-dark,
.vscode-dark pre code {
	color: #DDD;
}

.vscode-high-contrast,
.vscode-high-contrast pre code {
	color: white;
}

.vscode-light code {
	color: #A31515;
}

.vscode-dark code {
	color: #D7BA7D;
}

.vscode-light pre:not(.hljs),
.vscode-light code > div {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre:not(.hljs),
.vscode-dark code > div {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre:not(.hljs),
.vscode-high-contrast code > div {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

.vscode-light blockquote,
.vscode-dark blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.vscode-high-contrast blockquote {
	background: transparent;
	border-color: #fff;
}
</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family:  "Meiryo", "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

</head>
<body>
<h1 id="recupera%C3%A7%C3%A3o-e-minera%C3%A7%C3%A3o-de-texto-sobre-as-compras-governamentais">Recuperação e Mineração de Texto sobre as Compras Governamentais</h1>
<p><strong>Autor</strong>: Rafael Odon de Alencar</p>
<p><strong>Email</strong>: odon.rafael@gmail.com</p>
<p><strong>Data</strong>: 15/11/2018</p>
<h2 id="introdu%C3%A7%C3%A3o">Introdução</h2>
<p>O presente trabalho busca exercitar técnicas de recuperação de informação e de mineração de texto através do desenvolvimento de um sistema que coleta, extrai, processa e analisa sob determinadas óticas o conteúdo textual descritivo de uma amostra das compras do Governo Federal.
Os dados observados encontram-se disponíveis publicamente no site de <a href="http://compras.dados.gov.br">http://compras.dados.gov.br</a>.</p>
<p>As compras feitas pelo governo podem ser do tipo <strong>com licitação</strong> ou <strong>sem licitação</strong>, e são categorizadas com ajuda de um catálogo de <strong>serviços</strong> e <strong>materiais</strong> que agrupa compras de segmentos semelhantes. No enatnto, o volume de informações e a complexidade da base torna difícil contemplar as características gerais do comportamento de compra por parte das entidades públicas.</p>
<p>Afim de demonstrar o potencial de uma ferramenta automatizada para auxiliar na recuperação e análise em torno do texto dessas compras, foram selecionados apenas 2 serviços específicos do catálogo:</p>
<ul>
<li>Serviço 17663: Curso Aperfeiçoamento / Especialização Profissional</li>
<li>Serviço 3239: Transporte Rodoviário - Pessoal por Automóveis</li>
</ul>
<p>Em resumo, foram coletadas todas compras com e sem licitação desses dois serviços, e o conteúdo textual descritivo desses documentos foi extraído, processado e utilizado para gerar <em>insights</em>. As compras foram classificadas quanto à faixa de gasto a partir de uma análise da estatística descritiva. Em seguida, foram geradas nuvens de palavras destacando os termos descritivos de maior frequência para o <strong>grupo de gastos menores</strong> e para o <strong>grupo de gastos maiores</strong>. Um modelo de classificação <em>Naive Bayes</em> foi utilizado para verificar os termos que mais contribuiram para discriminar cada uma dessas classes. Também foi aplicada a técnica LDA (<em>Latent Dirichlet Allocation</em>) de detecção de tópicos em cada um desses grupos para verificar a co-ocorrência de termos nos conjunto de documentos. Por fim, uma estratégia de detecção de compras suspeitas foi proposta.</p>
<p>O sistema foi construído em Python 3.5 com ajudas de bibliotecas tais como <em>Pandas</em>, <em>Nltk</em>, <em>Scikit-Learn</em>, <em>Gensim</em>, <em>Matplotlib</em>, <em>Wordcloud</em> dentre outras. O código foi separado em classes conforme as responsabilidades do fluxo de trabalho: <strong>Coletor</strong>, <strong>Extrator</strong>, <strong>Processador</strong> e <strong>Analisador</strong>.</p>
<p>Cada uma das etapas será melhor descrita nas seções seguintes, bem como as observações e conclusões obtidas após as análises feitas.</p>
<h2 id="coleta-dos-documentos">Coleta dos documentos</h2>
<p>O site http://compras.dados.gov.br não só permite navegar pelos dados através de sua interface em HTML, mas também oferece APIs que retornam documentos Json.
Há uma <a href="http://compras.dados.gov.br/docs/lista-metodos-licitacoes.html">API própria para as licitações</a>, e outra <a href="http://compras.dados.gov.br/docs/lista-metodos-compraSemLicitacao.html">API própria para as compras sem licitação</a>.
Ambas possuem características diferentes mas permitem igualmente consultar uma numerosa lista paginada com todas as compras de um determinado serviço. Em ambas as APIs, uma compra pode envolver mais de um item, e assim é preciso também fazer novos acessos para encontrar os detalhes textuais daquele item.</p>
<p>Foi desenvolvida uma estratégia de coletada automatizada que busca todas as compras e licitações de um determinado serviço, com seus respectivos itens.
A classe <strong>Coletor</strong> é responsável por essa parte do fluxo de trabalho, navegando pelas APIs, indo para as próximas páginas quando essas existem e guardando todas as respostas Json obtidas num <strong>diretório de cache</strong>.
Dessa forma, ao ser re-executada, as compras já coletadas não são re-visitadas. Com isso, uma vez finalizada a coleta de todas as respostas das compras de um serviço é possível trabalhar <em>offline</em> sem a necessidade de acessar a API novamente.</p>
<p>Foi executada a coleta tanto para o <a href="http://compras.dados.gov.br/servicos/doc/servico/17663">serviço 17663</a> (Curso Aperfeiçoamento / Especialização Profissional) quanto para o <a href="http://compras.dados.gov.br/servicos/doc/servico/3239">serviço 3239</a> (Transporte Rodoviário - Pessoal por Automóveis).
Ao fim das coletas, constaram mais de 40 mil arquivos JSON no direótório de cache. Novos serviços podem ser coletados se houver interesse.</p>
<p>Além da navegação nas APIs de compras, também foi feita uma coleta simples da página de divulgação oficial da taxa SELIC (https://www.bcb.gov.br/pec/copom/port/taxaselic.asp), afim de subsidiar a atualização monetária dos valores das compras durante a análise.</p>
<p>As coletas ocorreram entre 30/10/2018 e 15/11/2018.</p>
<h2 id="extra%C3%A7%C3%A3o-de-dados">Extração de dados</h2>
<p>Mediante a coleta finalizada dos documentos de um serviço, o <strong>Extrator</strong> é responsável por fazer o <em>parse</em> dos documentos coletados, organizando as informações em um banco de dados relacional SQLite3 que torna fácil consultar as informações dos documentos.</p>
<p>Foram extraídos e armazenados como registros de uma tabela de documentos os seguintes dados:</p>
<ul>
<li>Id da Compra</li>
<li>Id do Serviço</li>
<li>Texto descritivo da compra</li>
<li>Texto descritivos dos itens da compra</li>
<li>Valor da compra (DOUBLE)</li>
<li>Data da compra (DATE)</li>
<li>Tipo (com licitação / sem licitação)</li>
</ul>
<p>Após a extração dos dados das compras, o banco de dados apresentou 3396 documentos do serviço 17663 (especialização) e 1842 documentos do serviço 3239 (transporte rodoviário).</p>
<p>Cada serviço tem os seus dados armazenados num banco separado, facilitando o manuseio nas etapas posteriores, já que as análises planejadas são feitas separadamente por serviço. A  presença da coluna <strong>Tipo</strong> torna o banco preparado para ser multi-serviço se necessáario. Ademais, a re-execução do <strong>Extrator</strong> apaga o banco e cria um novo, mas há como desligar essa abordagem.</p>
<p>O extrator também é responsável por fazer o <em>scraping</em> do HTML da página com o histórico da taxa SELIC mensal desde 1997, recuperando as células da tabela através de expressões <em>XPath</em> e armazenando os dados  obtidos numa tabela onde consta:</p>
<ul>
<li>Data início (DATE)</li>
<li>Data fim (DATE)</li>
<li>Valor da taxa SELIC (DOUBLE)</li>
</ul>
<h2 id="processamento-dos-dados">Processamento dos dados</h2>
<p>O <strong>Processador</strong> assume a existência do banco de dados relacional SQLite3 fruto da execução do <strong>Extrator</strong>, e a partir dele cria novas colunas e arquivos de apoio com dados que irão subsidiar a análise.</p>
<p>A primeira responsabilidade do <strong>Processador</strong> é pré-processar o conteúdo textual de cada documento para tornar possível a criação de um bag-of-words mais otimizado que dará suporte às análises sobre os termos. O pré-processamento do texto incluiu:</p>
<ul>
<li>
<p><strong>Texto em minúsculo</strong> - Optou-se por tratar todas as palavras em minúsculo.</p>
</li>
<li>
<p><strong>Remoção de acentos</strong> - Foi verificado navegando nos documentos extraídos, há ocorrência de palavras iguais com e sem acentuação ao longo dos textos, o que prejudica a correta contagem da frequência dos termos.</p>
</li>
<li>
<p><strong>Remoção da pontuação</strong> - Como não houve necessidade de preservar as sentenças, todas as palavras ficaram separadas por um único espaço, facilitando tratamentos posteriores. Isso foi feito com ajuda de um <strong>RegexpTokenizer</strong> que trasnformou o texto numa lista de palavras, ignorando espaços adjacentes e pontuação.</p>
</li>
<li>
<p><strong>Remoção de tokens numéricos</strong> - Tokens apenas numéricos não foram incluídos no texto processado final pois não apresentaram benefício para a análise.</p>
</li>
<li>
<p><strong>Remoção de palavras do domínio</strong> - Algumas expressões específicas do assunto Compras Goveernamentais estavam presentes nos documentos mas não contribuiram para uma boa compreensão do conteúdo das compras através da frequência de teremos. Sendo assim alguns termos foram removidos:</p>
<pre class="hljs"><code><div>REMOVER = [ 'pregao eletronico', 'pregao', 'aquisicao', 'valor',
  'limite' 'licitacao', 'licitacao', 'justificativa', 'edital',
  'contratacao', 'fornecimento', 'prestacao', 'precos', 'preco',
  'formacao','empresa', 'servico', 'servicos', 'inscricao',
  'pagamento', 'taxa','para', 'objeto' ]
</div></code></pre>
</li>
<li>
<p><strong>União de palavras quebradas:</strong>
Ao investigar a base de documentos visualmente, foi verificada uma grande ocorrência de palavras quebradas que deveriam estar unidas (ex: ca pacaitacação -&gt; capacitação, traba lho -&gt; trabalho).
Para tenta resolver esse problema, foi proposta uma heurística sobre a sequência de tokens do texto. Se o <em>token i</em> concatenado ao <em>token i+1</em> formar uma palavra uma palavra integrante do vocabulário composto por todos documentos em questão, cuja frequência dessa palavra unida seja maior que 25% da frequência dos tokens separados, então os 2 tokens adjacentes são transformados num único token concatenado.</p>
<p>Para que essa estratégia funcionasse foi preciso realizar uma primeira passada em todos os documentos para criar esse vocabulário e calcular as frequências dos tokens. Os resultados foram satisfatórios e trouxeram maior qualidade para a etapa de análise.</p>
<p>O trecho de LOG abaixo demonstra algumas uniões de palavras que aconteceram durante o processamento:</p>
<pre class="hljs"><code><div> ...
 2018-11-15 10:44:11,072 [DEBUG] - Unindo crit+erio
 2018-11-15 10:44:11,073 [DEBUG] - Unindo maqu+ina
 2018-11-15 10:44:11,099 [DEBUG] - Unindo mentori+ng
 2018-11-15 10:44:11,102 [DEBUG] - Unindo minis+trar
 2018-11-15 10:44:11,109 [DEBUG] - Unindo mer+cado
 2018-11-15 10:44:11,109 [DEBUG] - Unindo doce+ntes
 2018-11-15 10:44:11,109 [DEBUG] - Unindo integr+ada
 2018-11-15 10:44:11,109 [DEBUG] - Unindo oite+nta
 2018-11-15 10:44:11,126 [DEBUG] - Unindo univ+ersitaria
 2018-11-15 10:44:11,130 [DEBUG] - Unindo tra+nsferencia
 2018-11-15 10:44:11,132 [DEBUG] - Unindo amb+iente
 2018-11-15 10:44:11,137 [DEBUG] - Unindo enc+adernacao
 2018-11-15 10:44:11,142 [DEBUG] - Unindo w+indows
 2018-11-15 10:44:11,144 [DEBUG] - Unindo execut+iva
 2018-11-15 10:44:11,149 [DEBUG] - Unindo f+ormacao
 2018-11-15 10:44:11,152 [DEBUG] - Unindo vi+deo
 2018-11-15 10:44:11,163 [DEBUG] - Unindo te+cnicos    
 ...
</div></code></pre>
</li>
<li>
<p><strong>Stemming</strong> - Por fim, a sequẽncia de termos pré-processados é reduzida ao seu radical usando o <em>stemmer</em> para Português <strong>nltk.stem.RSLPStemmer</strong>. Como o objetivo era entender o panorama das compras governamentais de um determinado serviço, era importante também contemplar termos legíveis nas análises. Para tanto, ao realizar o <em>stemming</em>, foi armazenado num dicionário as frequências de cada variação do radical, para que num pós-processamento a top-palavra fosse utilizada como representante daquele conjunto de termos.</p>
<p>Abaixo segue uma entrada do dicionário de frequências:</p>
<pre class="hljs"><code><div>&quot;estim&quot;: {
    &quot;estimativas&quot;: 24,
    &quot;estimada&quot;: 4,
    &quot;estimados&quot;: 1,
    &quot;estimativa&quot;: 1,
    &quot;estimado&quot;: 3,
    &quot;estimadas&quot;: 1
},
</div></code></pre>
<p>No caso acima, a palavra <strong>estimativas</strong> é a melhor representante do radical <strong>estim</strong>, e será usada por exemplo para representar todas as demais palavras desse radical numa nuvem de palavra.</p>
</li>
</ul>
<p>As decisões de pré-processamento do texto acima descritam foram feitas iterativamente com as análises, principalmente observando a qualidade da nuvem de palavra. O pré-processamento foi primordial para gerar uma nuvem com menos 'sujeira' e com frequências mais significativas para determinados assuntos. Abaixo segue um exemplo do texto antes e depois do processamento:</p>
<table>
<thead>
<tr>
<th>Texto puro</th>
<th>Texto processsado</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Frete de veiculo no percurso redencao/kikretum/redencao.. Objeto: Pregão Eletrônico -  Contratação de emp resa especializada em serviço de instalação de linha de gases especiais.Justificativa: Conduzindo professores para a aldeia kikretum.</em></td>
<td><em>frete veiculos percurso redencao kikretum redencao empresa especializada instalacao linha gases especial conduzir professores aldeia kikretum</em></td>
</tr>
</tbody>
</table>
<p>O processamento final feito sobre o texto foi o ajuste de um objeto do tipo <strong>sklean.feature_extraction.text.TfidfVectorizer</strong> que recebeu como entrada o texto pré-processado de cada documento, e ajustou-se para fazer o cálculo do TF-IDF (Term Frequency - Inverse Document Frequncy), que é uma medida que traduz a frequência de um termo naquela coleção de documentos, levando em conta também que termos frequentes em mutos documentos são menos discriminantes.</p>
<p>O vetorizador foi configurado para trabalhar com 2000 palavras, cada uma sendo uma dimensão do <em>bag-of-words</em> final que pode representar um documento no espaço vetorial. O vetorizador também foi configurado para ignorar stopwords da língua Portuguesa através do <strong>nltk.corpus.stopwords</strong>.</p>
<p>Esse vetorizador foi serializado para ser usado durante a análise sempre que fosse necessário avaliar as frequências dos termos da coleção, ou vetorizar um conjunto de documentos.</p>
<p>Outra parte do processamento, não relacionada ao texto, foi o atualização monetária dos valores das compras pela taxa SELIC, permitindo assim uma análise mais justa das faixa de gasto maior e menor das compras durante a análise.</p>
<h2 id="an%C3%A1lise">Análise</h2>
<h3 id="defini%C3%A7%C3%A3o-das-faixas-de-gasto">Definição das faixas de gasto</h3>
<p>Uma análise estatística descritiva foi feito sobre os valores das compras de ambos os serviços. Em ambos os casos, verificou-se que, conforme é possível ver nas tabelas e nos gráficos, os valores de compras mais altos só ocorrem próximos do percentil 98, 99. Ou seja, a maior parte das compras tem valores moderados se comparadas com os valores máximos.</p>
<h4 id="tabela-1-estat%C3%ADstica-descritiva-do-servi%C3%A7o-17663-cursos">Tabela 1: estatística descritiva do serviço 17663 (Cursos)</h4>
<table>
<thead>
<tr>
<th>descritiva</th>
<th>valor</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>2765</td>
</tr>
<tr>
<td>mean</td>
<td>47737</td>
</tr>
<tr>
<td>std</td>
<td>389498</td>
</tr>
<tr>
<td>min</td>
<td>8</td>
</tr>
<tr>
<td>0%</td>
<td>8</td>
</tr>
<tr>
<td>15%</td>
<td>2002</td>
</tr>
<tr>
<td>30%</td>
<td>3384</td>
</tr>
<tr>
<td>45%</td>
<td>5499</td>
</tr>
<tr>
<td>60%</td>
<td>8522</td>
</tr>
<tr>
<td>75%</td>
<td>15915</td>
</tr>
<tr>
<td>90%</td>
<td>46917</td>
</tr>
<tr>
<td>93%</td>
<td>63320</td>
</tr>
<tr>
<td>96%</td>
<td>122307</td>
</tr>
<tr>
<td>99%</td>
<td>830910</td>
</tr>
<tr>
<td>max</td>
<td>15324904</td>
</tr>
</tbody>
</table>
<h4 id="figura-1---histograma-dos-valores-do-servi%C3%A7o-17663-cursos">Figura 1 - Histograma dos valores do Serviço 17663 (Cursos)</h4>
<p><img src="out/17663/histograma_valores.png?raw=true" alt=""></p>
<h4 id="tabela-1-estat%C3%ADstica-descritiva-do-servi%C3%A7o-3239-transporte">Tabela 1: estatística descritiva do serviço 3239 (Transporte)</h4>
<table>
<thead>
<tr>
<th>descritiva</th>
<th>valor</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>1011</td>
</tr>
<tr>
<td>mean</td>
<td>148983</td>
</tr>
<tr>
<td>std</td>
<td>1673170</td>
</tr>
<tr>
<td>min</td>
<td>7</td>
</tr>
<tr>
<td>0%</td>
<td>7</td>
</tr>
<tr>
<td>15%</td>
<td>1609</td>
</tr>
<tr>
<td>30%</td>
<td>2247</td>
</tr>
<tr>
<td>45%</td>
<td>4183</td>
</tr>
<tr>
<td>60%</td>
<td>7687</td>
</tr>
<tr>
<td>75%</td>
<td>18283</td>
</tr>
<tr>
<td>90%</td>
<td>68555</td>
</tr>
<tr>
<td>93%</td>
<td>126200</td>
</tr>
<tr>
<td>96%</td>
<td>442888</td>
</tr>
<tr>
<td>99%</td>
<td>2188247</td>
</tr>
<tr>
<td>max</td>
<td>43048801</td>
</tr>
</tbody>
</table>
<h4 id="figura-1---histograma-dos-valores-do-servi%C3%A7o-17663-cursos">Figura 1 - Histograma dos valores do Serviço 17663 (Cursos)</h4>
<p><img src="out/3239/histograma_valores.png?raw=true" alt=""></p>
<p>Para definir o ponto de corte da faixa de menor gasto para para a faixa de maior gasto, foram feitas algumas tentativas. Por fim, optou-se por uma regra que demonstrou chegar num valor adequado para ambos os serviços. O <strong>valor de corte</strong> foi definido como sendo a <strong>média</strong> somada com <strong>1 desvio</strong>.</p>
<p>Com essa abordagem, o ponto de corte do seviço 17663 foi o percentil 98.227848, e o do serviço 3239 foi o percentil 98.813056.</p>
<h3 id="frequ%C3%AAncia-de-termos">Frequência de termos</h3>
<p>O <strong>Analisador</strong> utiliza os valores TF-IDF advindos do processamento para gerar nuvens de palavras em que os termos com valores de frequências mais altas ficam em destaque. As nuvens foram construídas através do objeto <strong>wordcloud.WordCloud</strong> e foram criadas nuvens separadas para cada faixa de gasto, sendo que a <strong>Faixa 1</strong> é a faixa de menor gasto, e a <strong>Faixa 2</strong> é a faixa de maior gasto.</p>
<p>Também foram gerados na saída, arquivos contendo a lista de palavras mais frequentes, já que a nuvem, apesasr de visualmente atraente, não é muito boa para uma análise cautelosa comparativa entre os conjuntos de termos. (vide arquivos <strong>termos_Faixa1.md</strong> e <strong>termos_Faixa2.md</strong> nas pasta <strong>out</strong> )</p>
<h4 id="nuvens-de-palavras-do-servi%C3%A7o-17663-cursos-de-especializa%C3%A7%C3%A3o">Nuvens de palavras do serviço 17663 (Cursos de Especialização)</h4>
<p><img src="out/17663/tagcloud_Faixa1.png?raw=true" alt=""></p>
<p><img src="out/17663/tagcloud_Faixa2.png?raw=true" alt=""></p>
<p>É possível observar nas nuvens do serviço 17663 que grande parte das compras se relacionam de fato com investimento em aperfeiçoamento de servidores, sendo eles através de cursos, treinamentos, especializaçõões, materiais educativos, congressos. Destaca-se na nuvem da Faixa 2 o termo <strong>saúde</strong> que sugere que podem existir gastos mais elevados nessa área de formação.</p>
<h4 id="nuvens-de-palavras-do-servi%C3%A7o-3239-transporte-rodovi%C3%A1rio-de-pessoas">Nuvens de palavras do serviço 3239 (Transporte Rodoviário de Pessoas)</h4>
<p><img src="out/3239/tagcloud_Faixa1.png?raw=true" alt=""></p>
<p><img src="out/3239/tagcloud_Faixa2.png?raw=true" alt=""></p>
<p>As nuvens de palavras do serviço 3239 sugerem compras em torno de serviços de transporte, frete, contratação de motorista, aluguel de veículos. Mas chama a atenção na Faixa 1, de menor gasto, as palavras índios, indigena e aldeia, revelando um tipo de gasto governamental de pouca visibilidade.</p>
<h3 id="an%C3%A1lis-de-termos-discriminantes">Anális de Termos Discriminantes</h3>
<p>Afim de comprovar de fato os termos que discriminam uma faixa de gasto da outra, surgiu a idéia de ajustar um classificador <em>Naive Bayes</em> para que as informações do cálculo das probabilidades de cada classe pudesse ser usado como um ranking das palavras mais importantes. Dessa forma, foi utilizado o <strong>sklearn.naive_bayes.GaussianNB</strong>, primeiramente fazendo um treino com validação cruzada de 5 <em>folds</em> para medir sua acurácia, e em seguida ajustando-se com toda a base para gerar a informação probabilística.</p>
<p>Foi observado a informação da variância (<em>sigma</em>) e da média (<em>theta</em>)da feature por classe para rankear as top-palavras de cada uma das faixas de gasto.</p>
<h4 id="top-20-palavras-mais-discriminantes-para-a-faixa-1-do-servi%C3%A7o-17663">Top-20 palavras mais discriminantes para a Faixa 1 do serviço 17663:</h4>
<table>
<thead>
<tr>
<th>ranking</th>
<th>palavra</th>
<th>sigma</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>servidores</td>
<td>0.020315</td>
<td>0.128345</td>
</tr>
<tr>
<td>2</td>
<td>participacao</td>
<td>0.015974</td>
<td>0.048399</td>
</tr>
<tr>
<td>3</td>
<td>referente</td>
<td>0.015537</td>
<td>0.045957</td>
</tr>
<tr>
<td>4</td>
<td>periodo</td>
<td>0.012891</td>
<td>0.065668</td>
</tr>
<tr>
<td>5</td>
<td>brasilia</td>
<td>0.012198</td>
<td>0.040523</td>
</tr>
<tr>
<td>6</td>
<td>maria</td>
<td>0.010902</td>
<td>0.029835</td>
</tr>
<tr>
<td>7</td>
<td>realizado</td>
<td>0.010624</td>
<td>0.046874</td>
</tr>
<tr>
<td>8</td>
<td>lei</td>
<td>0.010547</td>
<td>0.033297</td>
</tr>
<tr>
<td>9</td>
<td>material</td>
<td>0.009802</td>
<td>0.052602</td>
</tr>
<tr>
<td>10</td>
<td>aperfeicoamento</td>
<td>0.009550</td>
<td>0.027284</td>
</tr>
<tr>
<td>11</td>
<td>registro</td>
<td>0.008832</td>
<td>0.033202</td>
</tr>
<tr>
<td>12</td>
<td>manutencao</td>
<td>0.008363</td>
<td>0.029003</td>
</tr>
<tr>
<td>13</td>
<td>horas</td>
<td>0.008012</td>
<td>0.020143</td>
</tr>
<tr>
<td>14</td>
<td>empresa</td>
<td>0.007952</td>
<td>0.028426</td>
</tr>
<tr>
<td>15</td>
<td>participar</td>
<td>0.007557</td>
<td>0.026616</td>
</tr>
<tr>
<td>16</td>
<td>atender</td>
<td>0.007556</td>
<td>0.041355</td>
</tr>
<tr>
<td>17</td>
<td>consumo</td>
<td>0.007145</td>
<td>0.021562</td>
</tr>
<tr>
<td>18</td>
<td>paulo</td>
<td>0.007127</td>
<td>0.020399</td>
</tr>
<tr>
<td>19</td>
<td>equipamentos</td>
<td>0.006857</td>
<td>0.026778</td>
</tr>
<tr>
<td>20</td>
<td>estabelecido</td>
<td>0.006670</td>
<td>0.026210</td>
</tr>
<tr>
<td>21</td>
<td>deste</td>
<td>0.006634</td>
<td>0.025207</td>
</tr>
<tr>
<td>22</td>
<td>nacional</td>
<td>0.006415</td>
<td>0.020672</td>
</tr>
<tr>
<td>23</td>
<td>direta</td>
<td>0.006321</td>
<td>0.015083</td>
</tr>
<tr>
<td>24</td>
<td>curso</td>
<td>0.006310</td>
<td>0.080502</td>
</tr>
<tr>
<td>25</td>
<td>mat</td>
<td>0.006262</td>
<td>0.012909</td>
</tr>
<tr>
<td>26</td>
<td>sao</td>
<td>0.005792</td>
<td>0.018641</td>
</tr>
<tr>
<td>27</td>
<td>conforme</td>
<td>0.005789</td>
<td>0.030746</td>
</tr>
<tr>
<td>28</td>
<td>ser</td>
<td>0.005766</td>
<td>0.029811</td>
</tr>
<tr>
<td>29</td>
<td>administracao</td>
<td>0.005748</td>
<td>0.018331</td>
</tr>
<tr>
<td>30</td>
<td>publica</td>
<td>0.005708</td>
<td>0.024211</td>
</tr>
</tbody>
</table>
<h4 id="top-20-palavras-mais-discriminantes-para-a-faixa-2-do-servi%C3%A7o-17663">Top-20 palavras mais discriminantes para a Faixa 2 do serviço 17663:</h4>
<table>
<thead>
<tr>
<th>ranking</th>
<th>palavra</th>
<th>sigma</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>treinamento</td>
<td>0.010706</td>
<td>0.039954</td>
</tr>
<tr>
<td>2</td>
<td>ambiental</td>
<td>0.009945</td>
<td>0.024885</td>
</tr>
<tr>
<td>3</td>
<td>graduacao</td>
<td>0.009906</td>
<td>0.038901</td>
</tr>
<tr>
<td>4</td>
<td>saude</td>
<td>0.009798</td>
<td>0.052138</td>
</tr>
<tr>
<td>5</td>
<td>pos</td>
<td>0.008984</td>
<td>0.032768</td>
</tr>
<tr>
<td>6</td>
<td>transito</td>
<td>0.007490</td>
<td>0.020014</td>
</tr>
<tr>
<td>7</td>
<td>componentes</td>
<td>0.007230</td>
<td>0.017397</td>
</tr>
<tr>
<td>8</td>
<td>educacao</td>
<td>0.006404</td>
<td>0.025989</td>
</tr>
<tr>
<td>9</td>
<td>horas</td>
<td>0.006333</td>
<td>0.021662</td>
</tr>
<tr>
<td>10</td>
<td>atender</td>
<td>0.005431</td>
<td>0.037598</td>
</tr>
<tr>
<td>11</td>
<td>producao</td>
<td>0.005213</td>
<td>0.014358</td>
</tr>
<tr>
<td>12</td>
<td>ufpe</td>
<td>0.005105</td>
<td>0.010312</td>
</tr>
<tr>
<td>13</td>
<td>servidores</td>
<td>0.005038</td>
<td>0.034969</td>
</tr>
<tr>
<td>14</td>
<td>naval</td>
<td>0.004922</td>
<td>0.010127</td>
</tr>
<tr>
<td>15</td>
<td>informatica</td>
<td>0.004836</td>
<td>0.018229</td>
</tr>
<tr>
<td>16</td>
<td>instituicao</td>
<td>0.004813</td>
<td>0.037921</td>
</tr>
<tr>
<td>17</td>
<td>capacitacao</td>
<td>0.004749</td>
<td>0.038096</td>
</tr>
<tr>
<td>18</td>
<td>humanos</td>
<td>0.004725</td>
<td>0.016365</td>
</tr>
<tr>
<td>19</td>
<td>recursos</td>
<td>0.004528</td>
<td>0.018485</td>
</tr>
<tr>
<td>20</td>
<td>especializacao</td>
<td>0.004423</td>
<td>0.041247</td>
</tr>
<tr>
<td>21</td>
<td>contratos</td>
<td>0.004229</td>
<td>0.023515</td>
</tr>
<tr>
<td>22</td>
<td>resfriamento</td>
<td>0.004215</td>
<td>0.009370</td>
</tr>
<tr>
<td>23</td>
<td>profissional</td>
<td>0.004155</td>
<td>0.029968</td>
</tr>
<tr>
<td>24</td>
<td>turma</td>
<td>0.004105</td>
<td>0.012278</td>
</tr>
<tr>
<td>25</td>
<td>seguranca</td>
<td>0.003964</td>
<td>0.009087</td>
</tr>
<tr>
<td>26</td>
<td>ensino</td>
<td>0.003936</td>
<td>0.023565</td>
</tr>
<tr>
<td>27</td>
<td>tecnica</td>
<td>0.003848</td>
<td>0.024772</td>
</tr>
<tr>
<td>28</td>
<td>interna</td>
<td>0.003832</td>
<td>0.013105</td>
</tr>
<tr>
<td>29</td>
<td>atualizacao</td>
<td>0.003766</td>
<td>0.018478</td>
</tr>
<tr>
<td>30</td>
<td>projeto</td>
<td>0.003713</td>
<td>0.022890</td>
</tr>
</tbody>
</table>
<p>É possível observar que na faixa 2 inclui assuntos como: ambiental, saúde, trânsito, informática, recursos humanos, segurança. Esses assuntos não constam na lista da Faixa 1. Isso pode sugerir que o compras com treinamento especializado em torno desses temas possam ser maiores ou mais numerosos no governo.</p>
<h4 id="top-20-palavras-mais-discriminantes-para-a-faixa-1-do-servi%C3%A7o-3239">Top-20 palavras mais discriminantes para a Faixa 1 do serviço 3239:</h4>
<table>
<thead>
<tr>
<th>ranking</th>
<th>palavra</th>
<th>sigma</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>materiais</td>
<td>0.050605</td>
<td>0.106390</td>
</tr>
<tr>
<td>2</td>
<td>indigenas</td>
<td>0.045294</td>
<td>0.087304</td>
</tr>
<tr>
<td>3</td>
<td>servicos</td>
<td>0.029950</td>
<td>0.061576</td>
</tr>
<tr>
<td>4</td>
<td>rodado</td>
<td>0.025620</td>
<td>0.042201</td>
</tr>
<tr>
<td>5</td>
<td>equipamentos</td>
<td>0.025571</td>
<td>0.055718</td>
</tr>
<tr>
<td>6</td>
<td>ate</td>
<td>0.024179</td>
<td>0.047951</td>
</tr>
<tr>
<td>7</td>
<td>veiculos</td>
<td>0.023904</td>
<td>0.053977</td>
</tr>
<tr>
<td>8</td>
<td>conforme</td>
<td>0.020759</td>
<td>0.066162</td>
</tr>
<tr>
<td>9</td>
<td>anexo</td>
<td>0.019888</td>
<td>0.062316</td>
</tr>
<tr>
<td>10</td>
<td>especializada</td>
<td>0.015370</td>
<td>0.043200</td>
</tr>
<tr>
<td>11</td>
<td>necessidades</td>
<td>0.014916</td>
<td>0.040655</td>
</tr>
<tr>
<td>12</td>
<td>atender</td>
<td>0.013932</td>
<td>0.050508</td>
</tr>
<tr>
<td>13</td>
<td>pessoal</td>
<td>0.012821</td>
<td>0.024324</td>
</tr>
<tr>
<td>14</td>
<td>locacao</td>
<td>0.012735</td>
<td>0.030054</td>
</tr>
<tr>
<td>15</td>
<td>manutencao</td>
<td>0.012541</td>
<td>0.032847</td>
</tr>
<tr>
<td>16</td>
<td>onibus</td>
<td>0.012337</td>
<td>0.024075</td>
</tr>
<tr>
<td>17</td>
<td>sendo</td>
<td>0.011998</td>
<td>0.028858</td>
</tr>
<tr>
<td>18</td>
<td>local</td>
<td>0.011329</td>
<td>0.024251</td>
</tr>
<tr>
<td>19</td>
<td>prestadora</td>
<td>0.010821</td>
<td>0.023853</td>
</tr>
<tr>
<td>20</td>
<td>medico</td>
<td>0.010401</td>
<td>0.018035</td>
</tr>
<tr>
<td>21</td>
<td>centro</td>
<td>0.009670</td>
<td>0.022158</td>
</tr>
<tr>
<td>22</td>
<td>hospital</td>
<td>0.009652</td>
<td>0.018050</td>
</tr>
<tr>
<td>23</td>
<td>sistema</td>
<td>0.009576</td>
<td>0.021816</td>
</tr>
<tr>
<td>24</td>
<td>tecnicas</td>
<td>0.009062</td>
<td>0.021343</td>
</tr>
<tr>
<td>25</td>
<td>rodoviario</td>
<td>0.008991</td>
<td>0.017003</td>
</tr>
<tr>
<td>26</td>
<td>pessoas</td>
<td>0.008973</td>
<td>0.023904</td>
</tr>
<tr>
<td>27</td>
<td>referencia</td>
<td>0.008864</td>
<td>0.027221</td>
</tr>
<tr>
<td>28</td>
<td>federal</td>
<td>0.008656</td>
<td>0.021758</td>
</tr>
<tr>
<td>29</td>
<td>diversos</td>
<td>0.008299</td>
<td>0.018132</td>
</tr>
<tr>
<td>30</td>
<td>termo</td>
<td>0.008096</td>
<td>0.027533</td>
</tr>
</tbody>
</table>
<h4 id="top-20-palavras-mais-discriminantes-para-a-faixa-2-do-servi%C3%A7o-3239">Top-20 palavras mais discriminantes para a Faixa 2 do serviço 3239:</h4>
<table>
<thead>
<tr>
<th>ranking</th>
<th>palavra</th>
<th>sigma</th>
<th>theta</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>diaria</td>
<td>0.033232</td>
<td>0.080719</td>
</tr>
<tr>
<td>2</td>
<td>horas</td>
<td>0.028042</td>
<td>0.074800</td>
</tr>
<tr>
<td>3</td>
<td>nuraf</td>
<td>0.022485</td>
<td>0.045212</td>
</tr>
<tr>
<td>4</td>
<td>meses</td>
<td>0.018017</td>
<td>0.068731</td>
</tr>
<tr>
<td>5</td>
<td>rodado</td>
<td>0.017411</td>
<td>0.039785</td>
</tr>
<tr>
<td>6</td>
<td>media</td>
<td>0.017411</td>
<td>0.039785</td>
</tr>
<tr>
<td>7</td>
<td>categoria</td>
<td>0.017411</td>
<td>0.039785</td>
</tr>
<tr>
<td>8</td>
<td>ano</td>
<td>0.017411</td>
<td>0.039785</td>
</tr>
<tr>
<td>9</td>
<td>pequenas</td>
<td>0.015892</td>
<td>0.081587</td>
</tr>
<tr>
<td>10</td>
<td>perimetro</td>
<td>0.015620</td>
<td>0.037683</td>
</tr>
<tr>
<td>11</td>
<td>irrigados</td>
<td>0.015620</td>
<td>0.037683</td>
</tr>
<tr>
<td>12</td>
<td>pecas</td>
<td>0.014804</td>
<td>0.060123</td>
</tr>
<tr>
<td>13</td>
<td>manutencao</td>
<td>0.014797</td>
<td>0.060058</td>
</tr>
<tr>
<td>14</td>
<td>locacao</td>
<td>0.014189</td>
<td>0.070948</td>
</tr>
<tr>
<td>15</td>
<td>salgado</td>
<td>0.013555</td>
<td>0.035103</td>
</tr>
<tr>
<td>16</td>
<td>internacional</td>
<td>0.013555</td>
<td>0.035103</td>
</tr>
<tr>
<td>17</td>
<td>filho</td>
<td>0.013555</td>
<td>0.035103</td>
</tr>
<tr>
<td>18</td>
<td>alegre</td>
<td>0.013555</td>
<td>0.035103</td>
</tr>
<tr>
<td>19</td>
<td>aeroporto</td>
<td>0.013555</td>
<td>0.035103</td>
</tr>
<tr>
<td>20</td>
<td>motorista</td>
<td>0.012552</td>
<td>0.093567</td>
</tr>
<tr>
<td>21</td>
<td>tipo</td>
<td>0.012027</td>
<td>0.062625</td>
</tr>
<tr>
<td>22</td>
<td>ser</td>
<td>0.011199</td>
<td>0.047283</td>
</tr>
<tr>
<td>23</td>
<td>ate</td>
<td>0.009923</td>
<td>0.056612</td>
</tr>
<tr>
<td>24</td>
<td>firma</td>
<td>0.009004</td>
<td>0.028611</td>
</tr>
<tr>
<td>25</td>
<td>veiculos</td>
<td>0.008929</td>
<td>0.115578</td>
</tr>
<tr>
<td>26</td>
<td>ans</td>
<td>0.008734</td>
<td>0.034988</td>
</tr>
<tr>
<td>27</td>
<td>pernoite</td>
<td>0.008520</td>
<td>0.031458</td>
</tr>
<tr>
<td>28</td>
<td>passageiros</td>
<td>0.007946</td>
<td>0.041299</td>
</tr>
<tr>
<td>29</td>
<td>cargas</td>
<td>0.007826</td>
<td>0.066090</td>
</tr>
<tr>
<td>30</td>
<td>taxi</td>
<td>0.007693</td>
<td>0.026445</td>
</tr>
</tbody>
</table>
<p>Dentre os termos mais discrimantes da Faixa 1 do serviço 3239, de fato está a palavra <strong>indígenas</strong>, comprovando que é um assunto em destaque dessa categoria. Nessa faixa surgem também as palavras <strong>médico</strong>, <strong>hospital</strong>, sugerindo deslocamentos de pessoas para fins de cuidados com a saúde. Já na Faixa 2, esses termos não ocorrem, e dão lugar à termos como <strong>aeroporto</strong>, <strong>taxi</strong>, <strong>pernoite</strong>, <strong>internacional</strong>, sugerindo uma temática ligada ao traslado de pessoas que viajam muito de avião.</p>
<h3 id="detec%C3%A7%C3%A3o-de-t%C3%B3%E1%B9%95icos">Detecção de Tóṕicos</h3>
<p>Para confirmar algumas das suspeitas de temas em torno das compras, foi executado o algoritmo do LDA que observa a co-ocorrência de palavras nos documentos, gerando grupos de palavras que juntas representam tópicos que podem resumir os assuntos mais tratados em uma coleção de documentos. Para tanto foi utilizada a biblioteca <strong>gensim</strong>.</p>
<p>O modelo de LDA foi ajustado para encontrar 3 tópicos com 10 passadas pela coleção de documentos de cada faixa de cada serviço. É sabido que o LDA é uma abordagem supevisionada, e logo, não é possível saber de antemão a quantidade de tópicos e de palavras nos tópicos que melhor representará os assuntos em torno da coleção. Dessa forma, também foi utilizada a biblioteca <strong>pyLDAvis</strong> que gera visualizações navegáveis em HTML dos tópicos obtidos. (vide arquivos <strong>lda.html</strong> na pasta <strong>out</strong>)</p>
<p>Para fins de insights, foram incluídos abaixo uma breve lista das palavras dos tópicos obtidos.</p>
<h4 id="t%C3%B3picos-identificados-pelo-lda-para-a-faixa-1-do-servi%C3%A7o-17663">Tópicos identificados pelo LDA para a Faixa 1 do serviço 17663:</h4>
<ul>
<li>
<p><strong>Tópico 1</strong>: 0.032*&quot;servidores&quot; + 0.029*&quot;curso&quot; + 0.013*&quot;material&quot; + 0.012*&quot;periodo&quot; + 0.008*&quot;realizado&quot; + 0.008*&quot;atender&quot; + 0.007*&quot;participacao&quot; + 0.006*&quot;brasilia&quot; + 0.006*&quot;capacitacao&quot; + 0.006*&quot;abaixo&quot;</p>
</li>
<li>
<p><strong>Tópico 2</strong>: 0.014*&quot;servidores&quot; + 0.010*&quot;periodo&quot; + 0.009*&quot;curso&quot; + 0.008*&quot;congresso&quot; + 0.007*&quot;brasilia&quot; + 0.007*&quot;memo&quot; + 0.007*&quot;silva&quot; + 0.006*&quot;atender&quot; + 0.006*&quot;realizado&quot; + 0.006*&quot;maria&quot;</p>
</li>
<li>
<p><strong>Tópico 3</strong>: 0.029*&quot;curso&quot; + 0.010*&quot;especializacao&quot; + 0.009*&quot;conforme&quot; + 0.008*&quot;anexo&quot; + 0.008*&quot;ser&quot; + 0.007*&quot;atender&quot; + 0.007*&quot;especializada&quot; + 0.006*&quot;servidores&quot; + 0.005*&quot;aperfeicoamento&quot; + 0.005*&quot;profissional&quot;</p>
</li>
</ul>
<h4 id="t%C3%B3picos-identificados-pelo-lda-para-a-faixa-2-do-servi%C3%A7o-17663">Tópicos identificados pelo LDA para a Faixa 2 do serviço 17663:</h4>
<ul>
<li>
<p><strong>Tópico 1</strong>: 0.023*&quot;curso&quot; + 0.015*&quot;saude&quot; + 0.015*&quot;treinamento&quot; + 0.010*&quot;atender&quot; + 0.010*&quot;especializacao&quot; + 0.009*&quot;horas&quot; + 0.008*&quot;material&quot; + 0.007*&quot;ser&quot; + 0.007*&quot;capacitacao&quot; + 0.006*&quot;profissional&quot;</p>
</li>
<li>
<p><strong>Tópico 2</strong>: 0.033*&quot;curso&quot; + 0.021*&quot;transito&quot; + 0.017*&quot;tecnica&quot; + 0.014*&quot;elaboracao&quot; + 0.014*&quot;area&quot; + 0.012*&quot;destinado&quot; + 0.011*&quot;sistema&quot; + 0.010*&quot;aplicacao&quot; + 0.010*&quot;capacitacao&quot; + 0.009*&quot;gestores&quot;</p>
</li>
<li>
<p><strong>Tópico 3</strong>: 0.025*&quot;curso&quot; + 0.015*&quot;educacao&quot; + 0.014*&quot;graduacao&quot; + 0.013*&quot;ambiental&quot; + 0.013*&quot;material&quot; + 0.011*&quot;capacitacao&quot; + 0.010*&quot;instituicao&quot; + 0.010*&quot;processo&quot; + 0.009*&quot;pos&quot; + 0.009*&quot;servidores&quot;</p>
</li>
</ul>
<p>No geral os tópicos giram em torno da capactiação de servidores, mas na Faixa 2 é possível de fato observar temática <strong>saúde</strong> no 1º tópico, <strong>gestores</strong> no 2º e <strong>ambiental</strong> no 3º.</p>
<h4 id="t%C3%B3picos-identificados-pelo-lda-para-a-faixa-1-do-servi%C3%A7o-3239">Tópicos identificados pelo LDA para a Faixa 1 do serviço 3239:</h4>
<ul>
<li>
<p><strong>Tópico 1</strong>: 0.030*&quot;frete&quot; + 0.028*&quot;aldeia&quot; + 0.023*&quot;memo&quot; + 0.021*&quot;transporte&quot; + 0.016*&quot;barra&quot; + 0.014*&quot;percurso&quot; + 0.013*&quot;indios&quot; + 0.012*&quot;aerbgs&quot; + 0.012*&quot;ate&quot; + 0.012*&quot;sao&quot;</p>
</li>
<li>
<p><strong>Tópico 2</strong>: 0.017*&quot;transporte&quot; + 0.013*&quot;frete&quot; + 0.009*&quot;atender&quot; + 0.009*&quot;memo&quot; + 0.008*&quot;onibus&quot; + 0.008*&quot;conforme&quot; + 0.007*&quot;anexo&quot; + 0.006*&quot;materiais&quot; + 0.006*&quot;caramuru&quot; + 0.005*&quot;veiculos&quot;</p>
</li>
<li>
<p><strong>Tópico 3</strong>: 0.036*&quot;transporte&quot; + 0.013*&quot;veiculos&quot; + 0.012*&quot;frete&quot; + 0.012*&quot;indigenas&quot; + 0.009*&quot;atender&quot; + 0.009*&quot;materiais&quot; + 0.006*&quot;pessoas&quot; + 0.006*&quot;conforme&quot; + 0.006*&quot;aldeia&quot; + 0.006*&quot;especializada&quot;</p>
</li>
</ul>
<h4 id="t%C3%B3picos-identificados-pelo-lda-para-a-faixa-2-do-servi%C3%A7o-3239">Tópicos identificados pelo LDA para a Faixa 2 do serviço 3239:</h4>
<ul>
<li>
<p><strong>Tópico 1</strong>: 0.060*&quot;veiculos&quot; + 0.046*&quot;motorista&quot; + 0.046*&quot;locacao&quot; + 0.046*&quot;pecas&quot; + 0.046*&quot;manutencao&quot; + 0.039*&quot;nuraf&quot; + 0.019*&quot;atender&quot; + 0.018*&quot;ans&quot; + 0.018*&quot;categoria&quot; + 0.018*&quot;media&quot;</p>
</li>
<li>
<p><strong>Tópico 2</strong>: 0.040*&quot;transporte&quot; + 0.036*&quot;pequenas&quot; + 0.029*&quot;veiculos&quot; + 0.026*&quot;cargas&quot; + 0.022*&quot;pessoas&quot; + 0.022*&quot;porto&quot; + 0.022*&quot;ate&quot; + 0.022*&quot;motorista&quot; + 0.019*&quot;meses&quot; + 0.019*&quot;especializada&quot;</p>
</li>
<li>
<p><strong>Tópico 3</strong>: 0.047*&quot;diaria&quot; + 0.035*&quot;tipo&quot; + 0.024*&quot;veiculos&quot; + 0.023*&quot;transporte&quot; + 0.023*&quot;meses&quot; + 0.023*&quot;inca&quot; + 0.023*&quot;horas&quot; + 0.021*&quot;feira&quot; + 0.021*&quot;unidades&quot; + 0.019*&quot;anexo&quot;</p>
</li>
</ul>
<p>No caso do serivço 3239, é possível verificar na Faixa 1 de fato a ocorrência de tópicos que incluem as palavras <strong>transporte</strong> juntamente com <strong>aldeia</strong>, <strong>índios</strong> e <strong>indígenas</strong>, reforçando a importância do tema na coleção. Já na Faixa 2, esses assuntos não ocorrem.</p>
<h3 id="dete%C3%A7%C3%A3o-de-compras-suspeitas">Deteção de compras suspeitas</h3>
<p>Com base nas análises anteriores, e assumindo ingenuamente que a análise dos textos das faixas de gasto de um serviço podem discriminar compras de alto valor de compras de baixo valor, sugere-se que um modelo de automático possa aprender a classificar uma compra quanto a sua faixa de gasto com base no seu texto, para em seguida classificar as compras coletadas e levantar suspeitas de comrpas que estão na Faixa 2 mas deveriam estar na Faixa 1.</p>
<p>Para tanto foi utilizado o classificador Random Forest com ajuda da classe <strong>sklearn.ensemble.RandomForestClassifier</strong>. Dado que temos 2000 features obtidas do vetorizador TF-IDF criado anteriormente, o classificador Random Forest foi testado e ajustado até apresentar uma alta acurácia, mas ainda sim gerando alguns falsos positivos para a Faixa 1, que serão tratados como suspeitos. A configuração configuração sugerido foi trabalhar com 5 árvores estimadoras e profundidade máxima 10 em cada árvore.</p>
<p>A acurácia foi verificada com validação cruzada de 5 <em>folds</em>, e em seguida toda a coleção foi classificada automaticamente. Os suspeitos foram listados em um arquivo separado, ordenados do maior valor de compra para o menor, além de oferecer o link para o detalhe da compra/licitação no site do governo. (vide o arquivo <strong>suspeitas.txt</strong> no diretório <strong>out</strong>)</p>
<h4 id="resultado-da-identifica%C3%A7%C3%A3o-de-compras-suspeitas-para-o-servi%C3%A7o-17663-cursos-de-especializa%C3%A7%C3%A3o">Resultado da identificação de compras suspeitas para o serviço 17663 (Cursos de Especialização):</h4>
<p>Classes:</p>
<ul>
<li>Faixa 1 (gasto até 437236.17) - 2716 registros.</li>
<li>Faixa 2 (acima de 437236.17) - 49 registros.</li>
</ul>
<p>Acurácias obtidas na validação cruzada com 5 folds: 98,19%, 98,19%, 98,01%, 98,19%, 98,36%.</p>
<p>Foram encontrada 41 suspeitas. Algumas delas:</p>
<ul>
<li>A compra #78 de valor 3723691.94 é da Faixa 2 mas parece ser da Faixa 1. (http://compras.dados.gov.br/compraSemLicitacao/doc/compra_slicitacao/20001206000012001)</li>
<li>A compra #2237 de valor 3359457.66 é da Faixa 2 mas parece ser da Faixa 1. (http://compras.dados.gov.br/compraSemLicitacao/doc/compra_slicitacao/15308006000101999)</li>
<li>A compra #2621 de valor 3054234.70 é da Faixa 2 mas parece ser da Faixa 1. (http://compras.dados.gov.br/compraSemLicitacao/doc/compra_slicitacao/25000506001022002)</li>
</ul>
<p>A primeira suspeita acima, da compra #78, foi avaliada no site do governo e apresentou dados de fato confusos. Sua descrição indica <em>&quot;prestação de serviços de limpeza e conservação prediais&quot;</em>. Já sua justificativa alega &quot;entidade Jurídica de direito privado, sem fins lucrativos, especializada para desenvolvimento dos cursos de qualificação&quot;. Ademais, a descrição do único item que compõe a compra é: &quot;realização de 50 cursos, atendendo 11 Estados da Federação, para qualificação de 2.000 (dois mil) Agentes Municipais de Trânsito, sendo 50 (cinquenta) turmas de 40 (quarenta) alunos cada.&quot; É importante destacar que pode também se tratar de um erro na base de dados.</p>
<h4 id="resultado-da-identifica%C3%A7%C3%A3o-de-compras-suspeitas-para-o-servi%C3%A7o-3239-transporte-rodovi%C3%A1rio-de-pessoas">Resultado da identificação de compras suspeitas para o serviço 3239 (Transporte Rodoviário de Pessoas):</h4>
<p>Classes:</p>
<ul>
<li>Faixa 1 (gasto até 1822154.05) - 999 registros.</li>
<li>Faixa 2 (acima de 1822154.05) - 12 registros.</li>
</ul>
<p>Acurácias obtidas na validação cruzada com 5 folds: 98,52%, 98,52%, 99,00%, 99,00%, 99,00%</p>
<p>Foram encontrada 2 suspeitas, conforme segue:</p>
<ul>
<li>A licitacao #1063 de valor 43048801.96 é da Faixa 2 mas parece ser da Faixa 1. (http://compras.dados.gov.br/licitacoes/doc/licitacao/25005203000892000)</li>
<li>A licitacao #1725 de valor 3260659.88 é da Faixa 2 mas parece ser da Faixa 1. (http://compras.dados.gov.br/licitacoes/doc/licitacao/25001502000012002)</li>
</ul>
<p>A primeira suspeita acima, da licitação #1063, foi avaliada no site do governo. É uma compra do Instituto Nacional do Câncer (INCA). Sua descrição é <em>&quot;Serviços de locação de diversos tipos de veículos, ambulâncias e caminhões, para atenderem às necessidades das diversas Unidades do INCA, conforme especificações constantes do edital.&quot;</em> Os item de compra dela são milhares de diárias de ambulâncias para transporte de pacientes. Um fato que pode ser observado nesse caso, é que outras variáveis devem ser levadas em conta pelo classificador de suspeitas além da informação textual. Uma delas é a quantidade e a unidade do item de compra, pois ela pode revelar dimensões importantes sobre a compra que está sendo feita, ajudando o classificador por exemplo a diferenciar uma compra cara e numerosa e uma compra barata de poucas unidades de um mesmo tipo de material/serviço.</p>
<h2 id="considera%C3%A7%C3%B5es-finais">Considerações Finais</h2>
<p>Foi apresentado um sistema automatizado de coleta, extração, processamento e análise do texto das compras governamentais, sob a das faixas de menor e maior gastos de um determinado serviço. A execução do fluxo para dois serviços específicos foi realizada e os resultados foram apresentados para ilustrar o potencial analítico do ferramental proposto. Foi possível exercitar diversos assuntos pertinentes aos tema recuperação da informação e mineração de textos.</p>
<p>O sistema demonstra alto grau de automatização, bastanto definir qual serviço será avaliado no arquivo <strong>constantes.py</strong>, e executando as etapas do fluxo através do arquivo <strong>main.py</strong>. As respostas da coleta são armazenadas no diretório <strong>cache</strong>, os dados intermediários do fluxo são armazenados no diretório <strong>data</strong>, separados por serviço. Os documentos e figuras frutos da análise são armazenados no diretório <strong>out</strong> separados por serviço.</p>
<p>O trabalho pode ser expandido tanto com a coleta de dados de novos serviços, quanto com o aprimoramento do sistema. Algumas melhorias futuras incluem: fazer a coleta de materiais (hoje só coleta serviços), incluir novas dimensões na análise além do texto (ex: levar em conta a data, o local, dados do órgão que originou a compra, dados do fornecedor que ofereceu o serviço/material, etc.), além de ajustes finos nas estratégias definidas.</p>
<p><em><strong>Fim</strong></em></p>

</body>
</html>
